{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import motion_tracking\n",
    "from random import randint\n",
    "import copy\n",
    "\n",
    "# plt.rcParams['figure.figsize'] = (15,5)\n",
    "# plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_path = \"sequence\"\n",
    "background_img = \"background.png\"\n",
    "\n",
    "# path = base path\n",
    "# folders = folders within path that contain images\n",
    "# Returns a list of strings of the relative path names for every image in path/ within every folder in folders\n",
    "def get_files(path):\n",
    "    file_list = list()\n",
    "    for (dir_path, dir_names, file_names) in os.walk(path):\n",
    "        file_list += [cv2.imread(os.path.join(dir_path, file)) for file in file_names]\n",
    "    return file_list\n",
    "\n",
    "images = get_files(sequence_path)  # loads and caches all the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pedestrian Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloDetector:\n",
    "    def __init__(self, frame_width, frame_height):\n",
    "        self.width = frame_width\n",
    "        self.height = frame_height\n",
    "        self.net = cv2.dnn.readNet('yolov3.weights','yolov3.cfg')\n",
    "        with open(\"coco.names\", \"r\") as f:\n",
    "            self.classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "        layer_names = self.net.getLayerNames()\n",
    "        self.output_layers = [layer_names[i[0] - 1] for i in self.net.getUnconnectedOutLayers()]\n",
    "        \n",
    "        #self.colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "        #self.font = cv2.FONT_HERSHEY_PLAIN\n",
    "        \n",
    "    def get_detection(self, img):\n",
    "        blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "        self.net.setInput(blob)\n",
    "        outs = self.net.forward(self.output_layers)\n",
    "\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        boxes = []\n",
    "        for out in outs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5:\n",
    "                    # Object detected\n",
    "                    center_x = int(detection[0] * self.width)\n",
    "                    center_y = int(detection[1] * self.height)\n",
    "                    w = int(detection[2] * self.width)\n",
    "                    h = int(detection[3] * self.height)\n",
    "                    # Rectangle coordinates\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "                    \n",
    "                    # Retain only pedestrians\n",
    "                    label = str(self.classes[class_id])\n",
    "                    if label == 'person':\n",
    "                        boxes.append([x, y, w, h])\n",
    "                        confidences.append(float(confidence))\n",
    "                        class_ids.append(class_id)\n",
    "        \n",
    "        # Remove overlapping duplicate boxes\n",
    "        best_boxes = []\n",
    "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "        for index_array in indexes:\n",
    "            best_boxes.append(boxes[index_array[0]])\n",
    "        return best_boxes\n",
    "        #return boxes, indexes, class_ids\n",
    "\n",
    "\n",
    "def draw_rectangles(boxes, img, color):\n",
    "    for i in range(len(boxes)):\n",
    "        x, y, w, h = boxes[i]\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "        #cv2.putText(img, label, (x, y + 30), font, 3, (255,0,0), 3)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pedestrian Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person:   \n",
    "    def __init__(self, frame_width, frame_height, num_particles):\n",
    "        self.bbox_history = [] # item: (x,y,w,h,frameID,isTracked)\n",
    "        self.tracker = ParticleFilter(frame_width, frame_height, num_particles)\n",
    "        \n",
    "        \n",
    "def get_optical_flow(prev_img_gray, cur_img_gray):\n",
    "    height,width = cur_img_gray.shape\n",
    "    hsv = np.zeros((height,width,3), dtype=cur_img_gray.dtype)\n",
    "    hsv[...,1] = 255\n",
    "    # optical flow (direction and velocity)\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_img_gray,cur_img_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
    "    hsv[...,0] = ang*180/np.pi/2\n",
    "    hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "    rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n",
    "    return mag, ang, rgb, flow\n",
    "\n",
    "def draw_flow(img, flow, step=16): # flow lines in direction of motion\n",
    "    h, w = img.shape[:2]\n",
    "    y, x = np.mgrid[step/2:h:step, step/2:w:step].reshape(2,-1).astype(int)\n",
    "    fx, fy = flow[y,x].T\n",
    "    lines = np.vstack([x, y, x+fx, y+fy]).T.reshape(-1, 2, 2)\n",
    "    lines = np.int32(lines + 0.5)\n",
    "    vis = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.polylines(vis, lines, 0, (0, 255, 0))\n",
    "    for (x1, y1), (_x2, _y2) in lines:\n",
    "        cv2.circle(vis, (x1, y1), 1, (0, 255, 0), -1)\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "height, width, channels = images[0].shape\n",
    "detector = YoloDetector(width, height)\n",
    "DETECT_WINDOW = 'Detection'\n",
    "FLOW_WINDOW = 'Flow'\n",
    "\n",
    "for n, cur_img in enumerate(images[1:]):\n",
    "    # setup\n",
    "    cur_img_gray = cv2.cvtColor(cur_img,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # 1. apply detector on new frame\n",
    "    detected_boxes = detector.get_detection(cur_img)\n",
    "\n",
    "    # 2. match detections to existing People\n",
    "    if n > 0:\n",
    "        # **** compute bbox velocities (pairs of frames)\n",
    "        flow_mag, flow_ang, flow_rgb, flow_res = get_optical_flow(prev_img_gray, cur_img_gray)\n",
    "        flow_arrow = draw_flow(cur_img_gray, flow_res, step=32)\n",
    "        # **** update predictions\n",
    "        # **** match prediction to closest bbox (use tracker if none found)\n",
    "        # **** update+resample trackers\n",
    "    # 3. create new People for unmatched bboxes\n",
    "    \n",
    "    # draw results\n",
    "    cur_img_display = copy.copy(cur_img)\n",
    "    draw_rectangles(detected_boxes, cur_img_display, (255,0,0))\n",
    "    cv2.putText(cur_img_display, f'Fm {n}', (0,20), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255,0,0),2);\n",
    "    \n",
    "    cv2.imshow(DETECT_WINDOW, cur_img_display)\n",
    "    \n",
    "    if n > 0:\n",
    "        cv2.imshow(FLOW_WINDOW, flow_arrow)\n",
    "    else:\n",
    "        cv2.imshow(FLOW_WINDOW, cur_img_display)\n",
    "    \n",
    "    # setup for next iteration\n",
    "    prev_img_gray = cur_img_gray\n",
    "    \n",
    "    # early exit\n",
    "    ch = cv2.waitKey(5) # millis delay\n",
    "    if ch == 27: # press ESC on the main window to exit\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_mag, flow_ang, flow_rgb, flow_res = get_optical_flow(prev_img_gray, cur_img_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(flow_res.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
