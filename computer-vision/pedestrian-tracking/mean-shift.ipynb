{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "import motion_tracking2 as motion_tracking\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import copy\n",
    "import math\n",
    "import traj_dist.distance as tdist\n",
    "import itertools\n",
    "import scipy.signal\n",
    "import numpy.linalg\n",
    "import pylab as py\n",
    "from collections import defaultdict\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
    "EPS = 1e-3 # float underflow tolerance\n",
    "PERSPECTIVE_TRANSFORM = np.array([\n",
    "        [1.51453582e+00, 5.20852351e+00, 6.64802632e+01],\n",
    "        [7.96413100e-02, 4.86708608e+00, 1.12297242e+01],\n",
    "        [4.72112167e-04, 2.16583992e-03, 6.11895338e-01]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_id corresponds to index of list\n",
    "# each list element is dict: key=object_iD, val=(x,y,w,h)\n",
    "# http://www.milanton.de/data/\n",
    "def get_ground_truths(is_center=False):\n",
    "    tree = ET.parse('PETS2009-S2L1.xml')\n",
    "    ground_truths = []\n",
    "    cur_frame = None\n",
    "    cur_object = None\n",
    "    root = tree.getroot()\n",
    "    for element in root.iter():  # hardcoded XML ordering\n",
    "        #print(\"%s - %s\" % (element.tag, element.attrib))\n",
    "        if element.tag == 'frame':\n",
    "            ground_truths.append({})\n",
    "            cur_frame = int(element.attrib['number'])\n",
    "\n",
    "        elif element.tag == 'object':\n",
    "            cur_object = int(element.attrib['id'])\n",
    "\n",
    "        elif element.tag == 'box':\n",
    "            xc, yc = float(element.attrib['xc']), float(element.attrib['yc'])\n",
    "            w, h   = float(element.attrib['w']), float(element.attrib['h']) \n",
    "            x, y   = xc-w/2, yc-h/2\n",
    "            if is_center:\n",
    "                bbox = (xc,yc,w,h)\n",
    "            else:\n",
    "                bbox = (x,y,w,h)\n",
    "            ground_truths[cur_frame][cur_object] = bbox\n",
    "    return ground_truths\n",
    "\n",
    "ground_truth = get_ground_truths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_path = \"sequence\"\n",
    "background_img = \"background.png\"\n",
    "\n",
    "# path = base path\n",
    "# folders = folders within path that contain images\n",
    "# Returns the opened images for every image in path/ within every folder in folders\n",
    "def get_files(path):\n",
    "    file_list = list()\n",
    "    for (dir_path, dir_names, file_names) in os.walk(path):\n",
    "        file_list += [cv2.imread(os.path.join(dir_path, file)) for file in file_names]\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = get_files(sequence_path)\n",
    "bg_img = cv2.imread(background_img)\n",
    "bg_img_gray = cv2.cvtColor(bg_img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_open = np.ones((3,3),np.uint8)\n",
    "k_close = np.ones((15,15), np.uint8)\n",
    "k_erode = np.ones((5,5),np.uint8)\n",
    "k_dilate = np.ones((5,5),np.uint8)\n",
    "\n",
    "def bg_subtract(img): # uses globals\n",
    "    #img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    subtract_1 = cv2.absdiff(bg_img, img)\n",
    "    \n",
    "    subtract = np.max(subtract_1, axis=2)\n",
    "    \n",
    "    ret, thresholded = cv2.threshold(subtract, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    morphed = cv2.morphologyEx(thresholded, cv2.MORPH_OPEN, k_open)\n",
    "    morphed = cv2.morphologyEx(morphed, cv2.MORPH_ERODE, k_erode)\n",
    "    morphed = cv2.morphologyEx(morphed, cv2.MORPH_DILATE, k_dilate)\n",
    "    morphed = cv2.morphologyEx(morphed, cv2.MORPH_CLOSE, k_close)\n",
    "    return morphed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pedestrian Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Detector Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloDetector:\n",
    "    def __init__(self, frame_width, frame_height):\n",
    "        self.width = frame_width\n",
    "        self.height = frame_height\n",
    "        self.net = cv2.dnn.readNet('yolov3.weights','yolov3.cfg')\n",
    "        with open(\"coco.names\", \"r\") as f:\n",
    "            self.classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "        layer_names = self.net.getLayerNames()\n",
    "        self.output_layers = [layer_names[i[0] - 1] for i in self.net.getUnconnectedOutLayers()]\n",
    "        \n",
    "        #self.colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "        #self.font = cv2.FONT_HERSHEY_PLAIN\n",
    "        \n",
    "    def get_detection(self, img):\n",
    "        blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "        self.net.setInput(blob)\n",
    "        outs = self.net.forward(self.output_layers)\n",
    "\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        boxes = []\n",
    "        for out in outs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5:\n",
    "                    # Object detected\n",
    "                    center_x = int(detection[0] * self.width)\n",
    "                    center_y = int(detection[1] * self.height)\n",
    "                    w = int(detection[2] * self.width)\n",
    "                    h = int(detection[3] * self.height)\n",
    "                    # Rectangle coordinates\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "                    \n",
    "                    # Retain only pedestrians\n",
    "                    label = str(self.classes[class_id])\n",
    "                    if label == 'person':\n",
    "                        boxes.append([x, y, w, h])\n",
    "                        confidences.append(float(confidence))\n",
    "                        class_ids.append(class_id)\n",
    "        \n",
    "        # Remove overlapping duplicate boxes\n",
    "        best_boxes = []\n",
    "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "        for index_array in indexes:\n",
    "            best_boxes.append(boxes[index_array[0]])\n",
    "        return best_boxes\n",
    "        #return boxes, indexes, class_ids\n",
    "\n",
    "\n",
    "def draw_rectangle(img, bbox, label, font, color,thickness=2):\n",
    "    cur_x, cur_y, cur_w, cur_h = bbox\n",
    "    img = cv2.rectangle(img, (cur_x, cur_y), (cur_x + cur_w, cur_y + cur_h), color, thickness)\n",
    "    if label != None:\n",
    "        img = cv2.putText(img, str(label), (cur_x+int(cur_w/2),cur_y-2), font, 0.5, color,thickness)\n",
    "    return img\n",
    "\n",
    "def draw_rectangles(boxes, img, color):\n",
    "    for i in range(len(boxes)):\n",
    "        x, y, w, h = boxes[i]\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "        #cv2.putText(img, label, (x, y + 30), font, 3, (255,0,0), 3)\n",
    "    return img\n",
    "\n",
    "def centroid(bbox):\n",
    "    x,y,w,h = bbox[:4]\n",
    "    return (x + int(w/2), y + int(h/2))\n",
    "\n",
    "def centroid_list(bboxes):\n",
    "    centroids = []\n",
    "    for i in range(len(bboxes)):\n",
    "        x,y,w,h = bboxes[i]\n",
    "        centroids.append((x + int(w/2), y + int(h/2)))\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pedestrian Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_histogram_feature(img, mask, bbox, is_norm): # color histogram\n",
    "    # if no bounded img is grabbed; bbox is no longer valid; insert black image\n",
    "    if motion_tracking.invalid_bbox(img,bbox):\n",
    "        bounded_img = np.zeros((21,21,3), dtype=np.uint8)\n",
    "        bounded_mask = np.zeros((21,21), dtype=np.uint8)\n",
    "        thres1, thres2 = 7, 14\n",
    "    else:\n",
    "        x,y,w,h = bbox[:4]\n",
    "        thres1, thres2 = int(h/3), int(h*2/3)\n",
    "        bounded_img = img[y:y+h, x:x+w]\n",
    "        bounded_mask = mask[y:y+h, x:x+w]\n",
    "    \n",
    "    hsv_img = cv2.cvtColor(bounded_img,cv2.COLOR_BGR2HSV)\n",
    "    if type(hsv_img) == type(None): # debug only\n",
    "        print(bounded_img.size, bbox, motion_tracking.clip_ords(img,bbox))\n",
    "    hsv_parts = [ hsv_img[:,:thres1], hsv_img[:,thres1:thres2], hsv_img[:,thres2:] ]\n",
    "    mask_parts = [ bounded_mask[:,:thres1], bounded_mask[:,thres1:thres2], bounded_mask[:,thres2:] ]\n",
    "    hist_parts = []\n",
    "    hist_contribution = np.zeros(len(hsv_parts))\n",
    "    \n",
    "    for i in range(len(hsv_parts)):\n",
    "        particle_subhist = cv2.calcHist([hsv_parts[i]], [0, 1], mask_parts[i], [180, 256], [0, 180, 0, 256])\n",
    "        particle_subhist_sum = particle_subhist.sum()\n",
    "        if is_norm and particle_subhist_sum > 0: # normalise by number of counted occurrences\n",
    "            particle_subhist /= particle_subhist_sum\n",
    "        hist_parts.append(particle_subhist)\n",
    "        hist_contribution[i] = particle_subhist_sum\n",
    "    return hist_parts, hist_contribution\n",
    "    # histogram(s) of img split as three columns; num masked entries in each subhist\n",
    "\n",
    "def random_color():\n",
    "    bright = [np.random.randint(215,256)] # note: one above the largest (signed) integer\n",
    "    dark = [np.random.randint(0,71)]\n",
    "    overall = [np.random.randint(0,256)] + bright + dark\n",
    "    np.random.shuffle(overall)\n",
    "    return tuple(map(lambda i: int(i), overall))\n",
    "\n",
    "def generic_text(img, text, loc):\n",
    "    img = cv2.putText(img, text, loc, FONT, 0.7, (0,0,0),4)\n",
    "    img = cv2.putText(img, text, loc, FONT, 0.7, (170,0,255),2)\n",
    "    return img\n",
    "    \n",
    "class Person:\n",
    "    # prediction params (constant velocity)\n",
    "    VELOCITY_WINDOW_FRAMES = 10\n",
    "    # tracking threshold params\n",
    "    INNER_BORDER_RECT = (398, 177, 67, 187) # (x,y,w,h) ROI rect between inner (lamppost) and middle regions\n",
    "    BORDER_PX = 20 # edge between middle and outer regions\n",
    "    CONSEC_TRACKED_FRAMES_THRESHOLD_INNER = 50\n",
    "    CONSEC_TRACKED_FRAMES_THRESHOLD_MIDDLE = 10\n",
    "    CONSEC_TRACKED_FRAMES_THRESHOLD_OUTER = 5\n",
    "    SUDDEN_FP_THRES = 10 # frames\n",
    "    # task-specific\n",
    "    TRAJECTORY_DRAW_LENGTH = 100\n",
    "    \n",
    "    def __init__(self, bbox, frame_width, frame_height, num_particles, person_id, img, mask, hc):\n",
    "        x,y,w,h,__,__ = bbox\n",
    "        self.person_id = person_id\n",
    "        self.bbox_history = [] # item: (x,y,w,h,frameID,isTracked)\n",
    "        self.consec_frames_tracked = 0\n",
    "        self.velocity_history = [] # item: (vx,vy,std,frameID)\n",
    "        self.color = random_color()\n",
    "        \n",
    "        #self.tracker = motion_tracking.ParticleFilter(frame_width, frame_height, num_particles)\n",
    "        self.tracker = motion_tracking.ParticleFilter(w, h, num_particles, x, y)\n",
    "        self.last_prediction = None # just: (x,y,w,h)\n",
    "        self.prev_normhist, self.prev_normhist_nentries = calc_histogram_feature(img, mask, bbox, True)\n",
    "        self.update_tracker(bbox, img, mask, hc, None, None, False)\n",
    "    \n",
    "    def get_latest_ords(self):\n",
    "        return self.bbox_history[-1]\n",
    "\n",
    "    def predict_tracker(self, vx, vy, std, img, frame_id):\n",
    "        self.velocity_history.append((vx,vy,std,frame_id))\n",
    "        max_index = len(self.velocity_history)-1\n",
    "        avg_vx, avg_vy, count = 0, 0, 0\n",
    "        for i in range(max_index, max(-1,max_index-self.VELOCITY_WINDOW_FRAMES), -1):\n",
    "            avg_vx += self.velocity_history[i][0]\n",
    "            avg_vy += self.velocity_history[i][1]\n",
    "            count += 1\n",
    "        avg_vx /= count\n",
    "        avg_vy /= count\n",
    "        #print(f'{frame_id}: p{self.person_id}, avg_vx={round(avg_vx,3)}, avg_vy={round(avg_vy,3)}')\n",
    "        self.tracker.predict(avg_vx,avg_vy,std) # shift the (unweighted) particles\n",
    "        ux, uy, __, __ = self.tracker.estimate() # weighted avg of particles\n",
    "        x,y,w,h,frame_id, is_tracked = self.get_latest_ords()\n",
    "        predicted_bbox = motion_tracking.clip_ords(img, (ux, uy, w, h))\n",
    "        self.last_prediction = predicted_bbox\n",
    "        return predicted_bbox\n",
    "    \n",
    "    def update_tracker(self,bbox, img,mask,hc, std_m,alpha, is_hist_method):\n",
    "        # add to bbox path history and miscell counters\n",
    "        x,y,w,h = motion_tracking.clip_ords(img, bbox)\n",
    "        __,__,__,__,frame_id, is_tracked = bbox\n",
    "        bbox = (x,y,w,h,frame_id,is_tracked) # clipped\n",
    "        self.bbox_history.append(bbox)\n",
    "        if is_tracked:\n",
    "            self.consec_frames_tracked += 1\n",
    "        else:\n",
    "            self.consec_frames_tracked = 0\n",
    "        \n",
    "        # interpolate with new histogram (of identical bins)\n",
    "        cur_hist, cur_normhist_nentries = calc_histogram_feature(img, mask, bbox, True)\n",
    "        for i in range(len(cur_hist)):\n",
    "            self.prev_normhist[i] = (1-hc) * cur_hist[i] + hc * self.prev_normhist[i]\n",
    "            self.prev_normhist[i][ self.prev_normhist[i] < EPS ] = 0\n",
    "            self.prev_normhist_nentries[i] = (1-hc) * cur_normhist_nentries[i] + hc * self.prev_normhist_nentries[i]\n",
    "        \n",
    "        # (computes the weights for the particles; does not do other book-keeping/side-effects)\n",
    "        if is_hist_method:\n",
    "            # update weights (from histogram)\n",
    "            wd = self.tracker.get_particle_detection_weights(x,y,std_m)\n",
    "            wc = self.tracker.get_color_appearance_weights(img,mask,w,h, self.prev_normhist,\n",
    "                                                           self.prev_normhist_nentries, \n",
    "                                                           calc_histogram_feature)\n",
    "            self.tracker.update_dual_weights(wd, wc, alpha)\n",
    "        else:\n",
    "            self.tracker.update(x,y) # update weights (from Euclidean)\n",
    "        self.tracker.resample(method='residual') # new set of particles\n",
    "    \n",
    "    def is_stale_tracker(self, img): # True --> stale\n",
    "        cx, cy, cw, ch, __, is_tracked = self.get_latest_ords()\n",
    "        cbbox = (cx,cy,cw,ch)\n",
    "        img_h, img_w = img.shape[:2]\n",
    "        # check if tracker is currently associated to a detection\n",
    "        if not is_tracked:\n",
    "            return False\n",
    "        # check if tracker bounds are invalid\n",
    "        if motion_tracking.invalid_bbox(img, cbbox):\n",
    "            return True\n",
    "        \n",
    "        # check if last detection occurred a while ago\n",
    "        # (threshold will be lower near edge of image)\n",
    "        consec_tracked_frames_threshold = self.CONSEC_TRACKED_FRAMES_THRESHOLD_MIDDLE\n",
    "        # ** inner-middle boundary (basically hardcoded)\n",
    "        pole_x0, pole_y0, pole_w, pole_h = self.INNER_BORDER_RECT\n",
    "        pole_x1, pole_y1 = pole_x0+pole_w, pole_y0+pole_h\n",
    "        if not ((cx+cw) < pole_x0 or (cy+ch) < pole_y0 or cx > pole_x1 or cy > pole_y1):\n",
    "            # not completely outside the inner boundary, so qualify as being inside\n",
    "            consec_tracked_frames_threshold = self.CONSEC_TRACKED_FRAMES_THRESHOLD_INNER\n",
    "        else: # (elif)\n",
    "            # ** middle-outer boundary\n",
    "            by0,by1 = self.BORDER_PX, img_h-self.BORDER_PX\n",
    "            bx0,bx1 = self.BORDER_PX, img_w-self.BORDER_PX\n",
    "            if cx < bx0 or cy < by0 or (cx+cw) > bx1 or (cy+ch) > by1:\n",
    "                # bbox is on the edge of the image\n",
    "                consec_tracked_frames_threshold = self.CONSEC_TRACKED_FRAMES_THRESHOLD_OUTER\n",
    "            \n",
    "        if self.consec_frames_tracked >= consec_tracked_frames_threshold:\n",
    "            return True\n",
    "        \n",
    "        # check if tracker was newly created on a false positive that has now disappeared\n",
    "        if len(self.bbox_history) <= self.SUDDEN_FP_THRES: \n",
    "            is_tracked_list = [b[-1] for i,b in enumerate(self.bbox_history) if i < self.SUDDEN_FP_THRES]\n",
    "            if is_tracked_list.count(True) > int(self.SUDDEN_FP_THRES/2):\n",
    "                return True # short early path, with majority of bboxes being tracked\n",
    "        return False # tracker not stale\n",
    "    \n",
    "    def __str__(self):\n",
    "        bbox = None\n",
    "        if len(self.bbox_history) > 0:\n",
    "            bbox = self.get_latest_ords()\n",
    "        return f'Person({bbox})'\n",
    "    \n",
    "    def draw(self, img, color=None, enable_particles=False, draw_last_predicted=False): # global FONT\n",
    "        if color == None:\n",
    "            color = self.color\n",
    "        if draw_last_predicted:\n",
    "            x,y,w,h = self.last_prediction\n",
    "        else:\n",
    "            x,y,w,h,__, __ = self.get_latest_ords()\n",
    "        img = cv2.rectangle(img, (x, y), (x + w, y + h), color, 1)\n",
    "        if enable_particles:\n",
    "            self.tracker.drawParticles(img, color, radius=1)\n",
    "        img = cv2.putText(img, str(self.person_id), (x+int(w/2),y-2), FONT, 0.5, color,2)\n",
    "        return img\n",
    "        \n",
    "    def draw_trajectory(self, img, color=None):\n",
    "        if color == None:\n",
    "            color = self.color\n",
    "        if len(self.bbox_history) <= 0:\n",
    "            return img\n",
    "        # draw backwards (from most recent location)\n",
    "        prev_center = centroid(self.get_latest_ords())\n",
    "        img = cv2.circle(img, prev_center, 1, color)\n",
    "        if len(self.bbox_history) <= 1:\n",
    "            return img\n",
    "        for count, i in enumerate(range(len(self.bbox_history)-1,0,-1)):\n",
    "            cur_center = centroid(self.bbox_history[i])\n",
    "            img = cv2.line(img, prev_center, cur_center, color, thickness=2)\n",
    "            prev_center = cur_center\n",
    "            if count >= self.TRAJECTORY_DRAW_LENGTH:\n",
    "                break\n",
    "        return img\n",
    "        \n",
    "def get_optical_flow(prev_img_gray, cur_img_gray):\n",
    "    height,width = cur_img_gray.shape\n",
    "    hsv = np.zeros((height,width,3), dtype=cur_img_gray.dtype)\n",
    "    hsv[...,1] = 255\n",
    "    # optical flow (direction and velocity)\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_img_gray,cur_img_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
    "    hsv[...,0] = ang*180/np.pi/2\n",
    "    hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "    rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n",
    "    return mag, ang, rgb, flow\n",
    "\n",
    "# https://stackoverflow.com/questions/27644388/optical-flow-using-opencv-horizontal-and-vertical-components\n",
    "def draw_flow(img, flow, step=16): # flow lines in direction of motion\n",
    "    h, w = img.shape[:2]\n",
    "    y, x = np.mgrid[step/2:h:step, step/2:w:step].reshape(2,-1).astype(int)\n",
    "    fx, fy = flow[y,x].T\n",
    "    lines = np.vstack([x, y, x+fx, y+fy]).T.reshape(-1, 2, 2)\n",
    "    lines = np.int32(lines + 0.5)\n",
    "    vis = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.polylines(vis, lines, 0, (0, 255, 0))\n",
    "    for (x1, y1), (_x2, _y2) in lines:\n",
    "        cv2.circle(vis, (x1, y1), 1, (0, 255, 0), -1)\n",
    "    return vis\n",
    "\n",
    "def get_values_from_mask(array_np, mask):\n",
    "    array_np = array_np.ravel()\n",
    "    mask = mask.ravel()\n",
    "    new_array = []\n",
    "    for i in range(mask.size):\n",
    "        if mask[i] > 0 or mask[i] == True:\n",
    "            new_array.append(array_np[i])\n",
    "    if len(new_array) == 0:\n",
    "        new_array = [0]\n",
    "    return new_array\n",
    "\n",
    "def get_velocity_stats(p, flow_res, mask=None): # tracker (for bbox); optical flow\n",
    "    # get velocity, but using box from prev frame\n",
    "    prev_bbox = p.get_latest_ords()\n",
    "    px,py,pw,ph, prev_n, is_tracked = prev_bbox\n",
    "    # use only the 'torso'\n",
    "    px = px + int(pw/4)\n",
    "    py = py + int(ph/4)\n",
    "    pw = int(pw/2)\n",
    "    ph = int(ph/4)\n",
    "    p_velocities = flow_res[py:(py+ph), px:(px+pw), :]\n",
    "    if p_velocities.size == 0:\n",
    "        return 0,0,0\n",
    "    x_velocities = p_velocities[:,:,0]\n",
    "    y_velocities = p_velocities[:,:,1]\n",
    "    if type(mask) == type(None):\n",
    "        cur_vx = np.average(x_velocities)\n",
    "        cur_vy = np.average(y_velocities)\n",
    "    else:\n",
    "        bounded_mask = mask[py:(py+ph), px:(px+pw)]\n",
    "        cur_vx = np.average(np.array(get_values_from_mask(x_velocities, bounded_mask)))\n",
    "        cur_vy = np.average(np.array(get_values_from_mask(y_velocities, bounded_mask)))\n",
    "    cur_std = np.std(p_velocities)\n",
    "    return cur_vx, cur_vy, cur_std\n",
    "\n",
    "#####[UNUSED]#############################\n",
    "# https://stackoverflow.com/questions/27152904/calculate-overlapped-area-between-two-rectangles\n",
    "def area_overlap(ref_bbox, target_bbox):\n",
    "    rx, ry, rw, rh = ref_bbox\n",
    "    tx, ty, tw, th = target_bbox\n",
    "    dx = min(rx+rw, tx+tw) - max(rx,tx)\n",
    "    dy = min(ry+rh, ty+th) - max(ry,ty)\n",
    "    if dx > 0 and dy > 0:\n",
    "        return dx * dy\n",
    "    return 0\n",
    "\n",
    "def ratio_overlap(ref_bbox, target_bbox): # should replace with IOU calc instead\n",
    "    rx, ry, rw, rh = ref_bbox\n",
    "    return area_overlap(ref_bbox, target_bbox) / (rw*rh)\n",
    "\n",
    "# assumes detection is flawless and generates only 1 bbox per pedestrian\n",
    "# usage: untracked_index = get_index_max_overlap(predicted_bbox, untracked_boxes, OVERLAP_THRESHOLD_RATIO)\n",
    "def get_index_max_overlap(ref_bbox, bbox_list, ratio_threshold):\n",
    "    if len(bbox_list) == 0:\n",
    "        return None\n",
    "    best_index = 0\n",
    "    best_area = area_overlap(ref_bbox, bbox_list[best_index])\n",
    "    for i in range(1,len(bbox_list)):\n",
    "        cur_area = area_overlap(ref_bbox, bbox_list[i])\n",
    "        if cur_area > best_area:\n",
    "            best_index = i\n",
    "            best_area = cur_area\n",
    "    # compare ratio (also filters out results with area = 0)\n",
    "    x,y,w,h = ref_bbox\n",
    "    best_ratio = best_area / (w*h)\n",
    "    if best_ratio >= ratio_threshold:\n",
    "        return best_index\n",
    "    return None\n",
    "#####[END OF UNUSED]######################\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/26363257/tracking-multiple-moving-objects-with-kalmanfilter-in-opencv-c-how-to-assign\n",
    "# http://www.hungarianalgorithm.com/examplehungarianalgorithm.php\n",
    "# https://stackoverflow.com/questions/28050678/algorithm-for-matching-point-sets\n",
    "def hungarian_algorithm(cur_people, detected_boxes, euclidean_thres):\n",
    "    # set up matrix (in same order as lists)\n",
    "    # ** cost = Euclidean distance\n",
    "    # ** row = trackers; col = detected boxes\n",
    "    matrix = np.zeros((len(cur_people), len(detected_boxes)))\n",
    "    for r in range(len(cur_people)):\n",
    "        px, py, pw, ph, __, __ = cur_people[r].get_latest_ords()\n",
    "        pxc, pyc = px+int(pw/2), py+int(ph/2)\n",
    "        for c in range(len(detected_boxes)):\n",
    "            # compute cost using centroid\n",
    "            bx, by, bw, bh = detected_boxes[c]\n",
    "            bxc, byc = bx+int(bw/2), by+int(bh/2)\n",
    "            matrix[r,c] = np.sqrt((pxc-bxc)**2 + (pyc-byc)**2)\n",
    "    # run min-cost (returns corresponding (r,c) pairs)\n",
    "    paired_tracker_idx, paired_detected_idx = linear_sum_assignment(matrix)\n",
    "    \n",
    "    # disassociate pairs that are physically too far away\n",
    "    idx_to_delete = []\n",
    "    for i in range(len(paired_tracker_idx)):\n",
    "        r = paired_tracker_idx[i]\n",
    "        c = paired_detected_idx[i]\n",
    "        if matrix[r,c] > euclidean_thres:\n",
    "            idx_to_delete.append(i)\n",
    "    paired_tracker_idx = np.delete(paired_tracker_idx, idx_to_delete)\n",
    "    paired_detected_idx = np.delete(paired_detected_idx, idx_to_delete)\n",
    "    \n",
    "    # find all unpaired trackers / detected boxes\n",
    "    unpaired_tracker_idx = np.setdiff1d(range(len(cur_people)), paired_tracker_idx)\n",
    "    unpaired_detected_idx = np.setdiff1d(range(len(detected_boxes)), paired_detected_idx)\n",
    "    return paired_tracker_idx, paired_detected_idx, unpaired_tracker_idx, unpaired_detected_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Group:\n",
    "    def __init__(self, people, gID):\n",
    "        # Keep list of people within the group\n",
    "        self.people = people\n",
    "        # Unique group ID for interframe association, and group dynamics analysis\n",
    "        self.gID = gID\n",
    "        # Store unique group colour for display purposes\n",
    "        self.color = random_color()\n",
    "    \n",
    "    def get_group_centroid(self):\n",
    "        cent = (0,0)\n",
    "        for i, p in enumerate(self.people):\n",
    "            p_cent = centroid(p.get_latest_ords()[:4])\n",
    "            c_x = cent[0] + p_cent[0]\n",
    "            c_y = cent[1] + p_cent[1]\n",
    "            cent = (c_x, c_y)\n",
    "        return (int(cent[0]/len(self.people)),int(cent[1]/len(self.people)))\n",
    "\n",
    "    # Returns average magnitude and angle of velocity over all people within group\n",
    "    def get_group_velocity(self):\n",
    "        mag = 0\n",
    "        ang = 0\n",
    "        for i, p in enumerate(self.people):\n",
    "            if len(p.bbox_history) > 1:\n",
    "                p_mag, p_ang = velocity(p.bbox_history[-2], p.bbox_history[-1], 1)\n",
    "            else:\n",
    "                p_mag, p_ang = 0, 0\n",
    "            mag = mag + p_mag\n",
    "            ang = ang + p_ang\n",
    "        return mag/len(self.people), ang/len(self.people)\n",
    "    \n",
    "#     def get_group_velocity(self):\n",
    "#         v = (0,0)\n",
    "#         for i, p in enumerate(self.people):\n",
    "#             if len(p.velocity_history) > 0:\n",
    "#                 vx, vy = velocity_components(p.bbox_history[-2], p.bbox_history[-1], 1)\n",
    "#             else:\n",
    "#                 vx, vy = 0, 0\n",
    "#             v = (v[0] + vx, v[1] + vy)\n",
    "#         return (v[0]/len(self.people), v[1]/len(self.people))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in an array of nx4 points, each entry of (cx, cy, vx, vy) corresponds to a single pedestrian\n",
    "# Outputs a pairwise euclidean centroid distance matrix, velocity distance matrix, and a list of IDs [for indexing pedestrian list]\n",
    "def dist_mats(pts):\n",
    "    d_c = np.zeros((len(pts),len(pts)))\n",
    "    d_v = np.zeros((len(pts),len(pts)))\n",
    "    ids = []\n",
    "    for p1, _ in enumerate(pts):\n",
    "#         ids.append(people[p1].person_id)\n",
    "        ids.append(p1)\n",
    "        for p2, _ in enumerate(pts):\n",
    "            dcx = pts[p1][0] - pts[p2][0]\n",
    "            dcy = pts[p1][1] - pts[p2][1]\n",
    "            dvx = pts[p1][2] - pts[p2][2]\n",
    "            dvy = pts[p1][3] - pts[p2][3]\n",
    "            \n",
    "            dist_c = math.sqrt((dcx*dcx) + (dcy*dcy))\n",
    "            dist_v = math.sqrt((dvx*dvx) + (dvy*dvy))\n",
    "            d_c[p1,p2] = dist_c\n",
    "            d_v[p1,p2] = dist_v\n",
    "    return d_c, d_v, ids\n",
    "\n",
    "\n",
    "# Takes in a list of nx2 points, each entry of nx2 points corresponds to 1 pedestrian's trajectory\n",
    "def traj_mat(traj_arr):\n",
    "    d = np.zeros((len(traj_arr),len(traj_arr)))\n",
    "    # Compute DTW distance between every pair of trajectories and add to distance matrix\n",
    "    for (m,n) in list((i,j) for ((i,_),(j,_)) in itertools.permutations(enumerate(traj_arr), 2)):\n",
    "        dtw_dist = tdist.dtw(np.transpose(traj_arr[m]),np.transpose(traj_arr[n]))\n",
    "        d[m,n] = 0.1*dtw_dist # Originally 0.2*dtw_dist\n",
    "    return d\n",
    "\n",
    "def vote_maj(adj):\n",
    "    return np.sum(adj, axis=0) >= 2\n",
    "def vote_unan(adj):\n",
    "    return np.sum(adj, axis=0) == 3\n",
    "def vote_weight(adj):\n",
    "    return (0.5*adj[0] + 0.5*adj[1] + 1*adj[2]) >= 1.5\n",
    "\n",
    "def graph_group(distances, cutoffs, ids):\n",
    "    adjacency = vote_weight([dist <= co for dist, co in zip(distances, cutoffs)])\n",
    "    unvisited = set([i for i in range(len(ids))])\n",
    "    queue = set({})\n",
    "    clusters = []\n",
    "    while len(unvisited) != 0 or len(queue) != 0:\n",
    "        if len(queue) == 0:\n",
    "            clusters.append(set({}))\n",
    "            queue.add(unvisited.pop())\n",
    "        curr = queue.pop()\n",
    "        clusters[-1].add(curr)\n",
    "        adj, = adjacency[curr].nonzero()\n",
    "        next_ids = set([i for i in adj if i in unvisited])\n",
    "        queue |= next_ids\n",
    "        unvisited -= next_ids\n",
    "    return [set(ids[list(x)]) for x in clusters]\n",
    "\n",
    "\n",
    "# Group pedestrians together\n",
    "def get_groups(people):\n",
    "    groups = []\n",
    "    pts = []\n",
    "    ppl = []\n",
    "    g_index = 0\n",
    "    c_arr = []\n",
    "    for n, p in enumerate(people):\n",
    "        length = len(p.bbox_history)\n",
    "        if (length < 3):\n",
    "            continue\n",
    "        if length % 2 == 0:\n",
    "            length = length - 1\n",
    "        \n",
    "        # TODO: Loosen requirement for 15 being necessary\n",
    "        # For filtering, require at least 15 bboxes\n",
    "#         if len(p.bbox_history) < 15:\n",
    "#             continue\n",
    "        bbox = p.bbox_history[-min(15, length):]\n",
    "        centroids = centroid_list(np.array(bbox)[:,:4])\n",
    "        # Correct the points\n",
    "        for i, c in enumerate(centroids):\n",
    "            centroids[i] = perspective_correction(c)\n",
    "        \n",
    "        # Get smoothed (filtered) centroids and velocities\n",
    "        (cx, cy, vx, vy) = smooth(np.array(centroids), min(15, length), 1)\n",
    "        c_list = np.array([0.05*cx,0.05*cy])\n",
    "        c_arr.append(c_list)\n",
    "        # Just need most recent values\n",
    "        cx = cx[-1]\n",
    "        cy = cy[-1]\n",
    "        vx = vx[-1]\n",
    "        vy = vy[-1]\n",
    "        \n",
    "        cx = 0.05*cx\n",
    "        cy = 0.05*cy\n",
    "        pt = (cx, cy, vx, vy)\n",
    "        pts.append(pt)\n",
    "        ppl.append(p)\n",
    "    if len(pts) < 2:\n",
    "        return []\n",
    "#     print(c_arr)\n",
    "\n",
    "    dist_c, dist_v, ids = dist_mats(pts)\n",
    "    dist_traj = traj_mat(c_arr)\n",
    "    \n",
    "    print([ppl[i].person_id for i in ids])\n",
    "    print(\"Centroid Distance: \")\n",
    "    print(dist_c)\n",
    "    print(\"Velocity Distance: \")\n",
    "    print(dist_v)\n",
    "    print(\"Trajectory Distance: \")\n",
    "    print(dist_traj)\n",
    "    \n",
    "    li = graph_group([np.array(dist_c), np.array(dist_v), np.array(dist_traj)], [4,2,2.5], np.array(ids))\n",
    "    print(li)\n",
    "    \n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    for i in li:\n",
    "        g_ppl_list = []\n",
    "        if len(i) < 2:\n",
    "            continue\n",
    "        for v in i:\n",
    "            # This needs to use ID of ppl list not pedestrian ID\n",
    "            g_ppl_list.append(ppl[v])\n",
    "        \n",
    "        # Create new group and add both centroids to it\n",
    "        groups.append([])\n",
    "        groups[g_index] = Group(g_ppl_list, 0)\n",
    "        g_index = g_index + 1\n",
    "    \n",
    "#     # Perform Mean-Shift fitting on this list\n",
    "#     MS = MeanShift(max_iter=300)\n",
    "#     clustering = MS.fit(np.array(pts))\n",
    "#     print(\"Estimated bandwidth: \", estimate_bandwidth(np.array(pts)))\n",
    "#     print(clustering.labels_)\n",
    "#     print(clustering.cluster_centers_)\n",
    "#     print(clustering.n_iter_)\n",
    "    \n",
    "    \n",
    "#     # THE FOLLOWING GROUPING LOGIC ISN'T CORRECT!\n",
    "#     d = defaultdict(list)\n",
    "#     # Iterate list with enumerate.\n",
    "#     for index, e in enumerate(clustering.labels_):\n",
    "#         d[e].append(index)\n",
    "    \n",
    "# #     print(d)\n",
    "    \n",
    "#     for k, v in d.items():\n",
    "#         # If there are not at least 2 pedestrians within a detected cluster, or the cluster is given label \"-1\" -> meaning they aren't within any clusters\n",
    "#         if len(v) < 2 or k == -1:\n",
    "#             continue\n",
    "#         print(k, \" has len >= 2\")\n",
    "#         l = []\n",
    "#         for i in v:\n",
    "#             # All ppl[i] here go in 1 group\n",
    "#             l.append(ppl[i])\n",
    "\n",
    "#         # Create new group and add both centroids to it\n",
    "#         groups.append([])\n",
    "#         groups[g_index] = Group(l, 0)\n",
    "#         g_index = g_index + 1\n",
    "    \n",
    "    # Ensures there are no duplicate people added (this is leftover from previous approach, probably not needed)\n",
    "#     for k in range(len(groups)):\n",
    "#         groups[k].people = list(set(groups[k].people))\n",
    "    return groups\n",
    "    \n",
    "\n",
    "# From list of n previous groups, determine what ID to give to current group\n",
    "def set_group_ids(groups, prev_groups, gID_count):\n",
    "    gID_list = []\n",
    "            \n",
    "    # if 1 person leaves group -> \n",
    "    # if 1 person joins group -> \n",
    "    # no change\n",
    "    # take largest group that is sub or superset\n",
    "    for g, _ in enumerate(groups):\n",
    "        g_id = -1\n",
    "        color = (0,0,0)\n",
    "        pIDs = set([p.person_id for p in groups[g].people])\n",
    "        for i in range(len(prev_groups)):\n",
    "            for j in range(len(prev_groups[i])):\n",
    "                pIDs_prev = set([p.person_id for p in prev_groups[i][j].people])\n",
    "    #             if all(item in pIDs for item in pIDs_prev):\n",
    "                if pIDs_prev.issuperset(pIDs) or pIDs.issuperset(pIDs_prev):\n",
    "                    g_id = prev_groups[i][j].gID\n",
    "                    color = prev_groups[i][j].color\n",
    "        if g_id == -1:\n",
    "            groups[g].gID = gID_count\n",
    "            gID_count = gID_count + 1\n",
    "        else:\n",
    "            groups[g].gID = g_id\n",
    "            groups[g].color = color\n",
    "        gID_list.append(g_id)\n",
    "    \n",
    "    # Now check duplicate groups\n",
    "    D = defaultdict(list)\n",
    "    for i, item in enumerate(gID_list):\n",
    "        D[item].append(i)\n",
    "    D = {k:v for k,v in D.items() if len(v)>1}\n",
    "    \n",
    "    # -1 represents a new group has been made so ignore these\n",
    "    if -1 in D:\n",
    "        D.pop(-1)\n",
    "\n",
    "    # For each duplicate ID found, find biggest group -> this group retains the ID, all others get set to a new Group\n",
    "    for i in D:\n",
    "        max_len = 0\n",
    "        best_j = 0\n",
    "        for j in D[i]:\n",
    "            l = len(groups[j].people)\n",
    "            if l > max_len:\n",
    "                max_len = l\n",
    "                best_j = j\n",
    "        for x in D[i]:\n",
    "            if x == best_j:\n",
    "                continue\n",
    "            # Remake any group that isn't the largest duplicate group\n",
    "            # Largest group will keep previous group ID\n",
    "            groups[x] = Group(groups[x].people, gID_count)\n",
    "            gID_count = gID_count + 1\n",
    "    return groups, gID_count\n",
    "\n",
    "\n",
    "# Using specification of 7 frames per second from dataset information\n",
    "def velocity(p1, p2, num_frames):\n",
    "    dx = p2[0] - p1[0]\n",
    "    dy = p2[1] - p1[1]\n",
    "    \n",
    "    # 7 fps\n",
    "    mag = 7 * math.sqrt((dx*dx) + (dy*dy)) / num_frames\n",
    "    angle = math.atan2(dy, dx)\n",
    "    return mag, angle\n",
    "\n",
    "def velocity_components(p1, p2, num_frames):\n",
    "    dx = p2[0] - p1[0]\n",
    "    dy = p2[1] - p1[1]\n",
    "    \n",
    "    return (dx/num_frames, dy/num_frames)\n",
    "\n",
    "def euclid_dist(p1, p2):\n",
    "    x = p1[0] - p2[0]\n",
    "    y = p1[1] - p2[1]\n",
    "    return math.sqrt((x*x) + (y*y))\n",
    "\n",
    "def euclid_dist_list(p1, p2):\n",
    "    dist = []\n",
    "    if len(p1) != len(p2):\n",
    "        return []\n",
    "    for i in range(len(p1)):\n",
    "        x = p1[i][0] - p2[i][0]\n",
    "        y = p1[i][1] - p2[i][1]\n",
    "        dist.append(math.sqrt((x*x) + (y*y)))\n",
    "    return dist\n",
    "\n",
    "# Use this function to apply transform to correct perspective shift\n",
    "def perspective_correction(pt):\n",
    "    pt = np.append(np.array(pt), 1).transpose()\n",
    "    pt = PERSPECTIVE_TRANSFORM @ pt\n",
    "    return (pt[0] / pt[2], pt[1] / pt[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in a list of centroids (c_x, c_y) -> must be at least 15 in the list\n",
    "# Outputs smoothed centroid values and velocity values (c_x, c_y, v_x, v_y)\n",
    "# Returns None if pos is not sufficient length\n",
    "\n",
    "# Best values found were window_size = 15, order = 2\n",
    "def smooth(pos, window_size, order):\n",
    "    if pos.shape[0] >= window_size:\n",
    "        z_x = scipy.signal.savgol_filter(pos[:,0], window_size, order, deriv=0, delta=1.0, axis=-1, mode='interp', cval=0.0)\n",
    "        z_y = scipy.signal.savgol_filter(pos[:,1], window_size, order, deriv=0, delta=1.0, axis=-1, mode='interp', cval=0.0)\n",
    "        z_vx = scipy.signal.savgol_filter(pos[:,0], window_size, order, deriv=1, delta=1.0, axis=-1, mode='interp', cval=0.0)\n",
    "        z_vy = scipy.signal.savgol_filter(pos[:,1], window_size, order, deriv=1, delta=1.0, axis=-1, mode='interp', cval=0.0)\n",
    "        return (z_x, z_y, z_vx, z_vy)\n",
    "    else:\n",
    "        raise ValueError(\"pos must be of at shape (n,2) where n >= \", window_size, \"!\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime Logic & UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "height, width, channels = images[0].shape\n",
    "detector = YoloDetector(width, height)\n",
    "cur_people = []\n",
    "cur_person_id = 0\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "framerate = 7\n",
    "video_out = cv2.VideoWriter(\"tracking-output.avi\", fourcc, framerate, (width,height))\n",
    "\n",
    "# Task 1b/c, Task 2 relevant params\n",
    "ENABLE_TASK_VIEW = True\n",
    "ENABLE_GROUND_TRUTH_DEBUG_VIEW = True\n",
    "\n",
    "# primarily: [TRACKING PARAMETERS]\n",
    "ENABLE_TRACKING_DEBUG_VIEW = False\n",
    "OVERLAP_THRESHOLD_RATIO = 0.4\n",
    "\n",
    "DETECT_WINDOW = 'Detection'\n",
    "DETECT_COLOR = (255,0,0) # blue\n",
    "UNTRACK_COLOR = (255,0,191) # purple\n",
    "TRACK_COLOR = (0,0,255) # red\n",
    "GROUND_TRUTH_COLOR = (0,0,0) # black (170,0,255) # pink\n",
    "\n",
    "FLOW_WINDOW = 'Flow'\n",
    "MASK_WINDOW = 'Mask'\n",
    "\n",
    "INTERACTIVE_CONTROL = True\n",
    "ESC_KEY = 27\n",
    "PAUSE_KEY = ord('p')\n",
    "NEXT_KEY = ord('n')\n",
    "KEY_LIST = [ESC_KEY, PAUSE_KEY, NEXT_KEY]\n",
    "\n",
    "STD_DEV = 10 # px\n",
    "CONST_hc = 0.3\n",
    "CONST_std_m = STD_DEV\n",
    "CONST_alpha_occlus = 0.6\n",
    "EUCLIDEAN_THRES = 100-25 # px, for Hungarian\n",
    "\n",
    "FRAME_START = 0 #460 #390 #128\n",
    "DRAW_PARTICLES = False\n",
    "\n",
    "# https://stackoverflow.com/questions/15933741/how-do-i-catch-a-numpy-warning-like-its-an-exception-not-just-for-testing\n",
    "np.seterr(all='raise') \n",
    "is_interactive = INTERACTIVE_CONTROL\n",
    "\n",
    "# Task 2 initialisation\n",
    "cur_img_display = copy.copy(images[FRAME_START])\n",
    "cur_img_display = generic_text(cur_img_display, 'Select ROI and press ENTER', (0,20))\n",
    "task2_roi = cv2.selectROI(DETECT_WINDOW, cur_img_display)\n",
    "task2_ids = set() # IDs inside the Task 2 ROI\n",
    "\n",
    "# Store previous 3 groups\n",
    "num_prev_frames = 1\n",
    "prev_groups = [None] * num_prev_frames\n",
    "\n",
    "gID_count = 0\n",
    "\n",
    "for n, cur_img in enumerate(images[FRAME_START:]):\n",
    "\n",
    "    ###############################\n",
    "    # [DETECTION & BACKGROUND SUBTRACTION]\n",
    "    # setup\n",
    "    cur_img_gray = cv2.cvtColor(cur_img,cv2.COLOR_BGR2GRAY)\n",
    "    bw_mask = bg_subtract(cur_img)\n",
    "    \n",
    "    # 1. apply detector on new frame\n",
    "    detected_boxes = detector.get_detection(cur_img)\n",
    "    \n",
    "    ###############################\n",
    "    # [TRACKING]\n",
    "    # 2. match detections to existing Person\n",
    "    if n > 0:\n",
    "        __, __, flow_rgb, flow_res = get_optical_flow(prev_img_gray, cur_img_gray)\n",
    "        # associate trackers (existing Person) to detected bboxes by distance\n",
    "        p_idx_list, d_idx_list, unp_idx_list, und_idx_list = hungarian_algorithm(cur_people, detected_boxes, EUCLIDEAN_THRES)\n",
    "        \n",
    "        # paired (trackers, detected)\n",
    "        for index_pair in range(len(p_idx_list)):\n",
    "            # update tracker to use detector's coords\n",
    "            p = cur_people[ p_idx_list[index_pair] ]\n",
    "            cur_x, cur_y, cur_w, cur_h = detected_boxes[ d_idx_list[index_pair] ]\n",
    "            # tracker prediction (results unused...)\n",
    "            cur_vx, cur_vy, cur_std = get_velocity_stats(p, flow_res, bw_mask)\n",
    "            p.predict_tracker(cur_vx, cur_vy, STD_DEV, cur_img, n)\n",
    "            # tracker update\n",
    "            detected_bbox = (cur_x, cur_y, cur_w, cur_h, n, False)\n",
    "            p.update_tracker(detected_bbox, cur_img, bw_mask, CONST_hc, None, None, False)\n",
    "            # NOTE: our detector is likely more accurate than our tracker; atm I don't know\n",
    "            # how to directly associate the detected bbox to the tracker with the\n",
    "            # color histogram approach?\n",
    "            \n",
    "        # unpaired trackers\n",
    "        for unpaired_index in unp_idx_list:  # (no index pairing required)\n",
    "            # predict\n",
    "            p = cur_people[unpaired_index]\n",
    "            cur_vx, cur_vy, cur_std = get_velocity_stats(p, flow_res, bw_mask)\n",
    "            predicted_bbox = p.predict_tracker(cur_vx, cur_vy, STD_DEV, cur_img, n)\n",
    "            predicted_bbox = tuple(list(predicted_bbox) + [n, True])\n",
    "            # update\n",
    "            p.update_tracker(predicted_bbox, cur_img, bw_mask, CONST_hc, \n",
    "                             CONST_std_m, CONST_alpha_occlus, True)\n",
    "            \n",
    "    else:\n",
    "        # unpaired detected bboxes (see Step 3.)\n",
    "        und_idx_list = list(range(len(detected_boxes))) # all undetected\n",
    "        p_idx_list = [] # to aid plotting\n",
    "        d_idx_list = []\n",
    "        unp_idx_list = []\n",
    "\n",
    "    # 3. create new Person for each unpaired bbox\n",
    "    unpaired_start_index = len(cur_people)\n",
    "    for unpaired_index in und_idx_list:  # (no index pairing required)\n",
    "        cur_x, cur_y, cur_w, cur_h = detected_boxes[ unpaired_index ]\n",
    "        p = Person( (cur_x,cur_y,cur_w,cur_h,n,False), width,height,100, \n",
    "                   cur_person_id, cur_img, bw_mask, CONST_hc)\n",
    "        cur_people.append(p)\n",
    "        cur_person_id += 1\n",
    "    \n",
    "    # NOTE: we remove stale trackers after plotting results, for coding convenience\n",
    "    \n",
    "    ###############################\n",
    "    # [PLOT TRACKING-ONLY RESULTS]\n",
    "    cur_img_display = copy.copy(cur_img)\n",
    "    if ENABLE_TRACKING_DEBUG_VIEW:\n",
    "        post_x, post_y, post_w, post_h = Person.INNER_BORDER_RECT\n",
    "        cur_img_display = cv2.rectangle(cur_img_display, (post_x, post_y), \n",
    "                                        (post_x+post_w, post_y+post_h), (0,0,0), 1)\n",
    "        cur_img_display = cv2.rectangle(cur_img_display, (Person.BORDER_PX, Person.BORDER_PX), \n",
    "                                        (width-Person.BORDER_PX, height-Person.BORDER_PX), (0,0,0), 1)\n",
    "#         cv2.putText(cur_img_display, f'Fm {FRAME_START+n}', (0,20), FONT, 0.75, (255,0,0),2)  # frame id\n",
    "    \n",
    "    # paired (trackers, detected)\n",
    "    for index_pair in range(len(p_idx_list)):\n",
    "        p = cur_people[ p_idx_list[index_pair] ]\n",
    "        cur_bbox = p.get_latest_ords()[:4]\n",
    "        #cur_bbox = detected_boxes[ d_idx_list[index_pair] ]\n",
    "        cur_img_display = draw_rectangle(cur_img_display, cur_bbox, str(p.person_id), FONT, DETECT_COLOR)\n",
    "    \n",
    "    # unpaired trackers\n",
    "    for unpaired_index in unp_idx_list:\n",
    "        p = cur_people[unpaired_index]\n",
    "        cur_bbox = p.get_latest_ords()[:4]\n",
    "        cur_img_display = draw_rectangle(cur_img_display, cur_bbox, str(p.person_id), FONT, TRACK_COLOR)\n",
    "    \n",
    "    # unpaired detected bboxes\n",
    "    for unpaired_index in range(unpaired_start_index, len(cur_people)):\n",
    "        p = cur_people[unpaired_index]\n",
    "        cur_bbox = p.get_latest_ords()[:4]\n",
    "        cur_img_display = draw_rectangle(cur_img_display, cur_bbox, str(p.person_id), FONT, UNTRACK_COLOR)\n",
    "        \n",
    "    # tracker predictions, particles\n",
    "    for p in cur_people:\n",
    "        if p.last_prediction == None: # i.e ignore new Person\n",
    "            continue\n",
    "        cur_img_display = p.draw(cur_img_display, TRACK_COLOR, \n",
    "                                 enable_particles=DRAW_PARTICLES, draw_last_predicted=True)\n",
    "    \n",
    "    ###############################\n",
    "    # [TRACKING CLEAN-UP]\n",
    "    # remove stale trackers\n",
    "    p_index = 0\n",
    "    while p_index < len(cur_people):\n",
    "        p = cur_people[p_index]\n",
    "        if p.is_stale_tracker(cur_img):\n",
    "            del cur_people[p_index]\n",
    "            continue\n",
    "        p_index += 1\n",
    "    \n",
    "#     ###############################\n",
    "    # [TASK 2 LOGIC]\n",
    "    task2_entered = 0\n",
    "    task2_left = 0\n",
    "    for p in cur_people: # probably a non-optimal extra loop\n",
    "        cur_bbox = p.get_latest_ords()[:4]\n",
    "        # 1. check for overlapping area (must be more than just a boundary touch)\n",
    "        # 2. check for existing membership\n",
    "        if area_overlap(cur_bbox, task2_roi) > 0:\n",
    "            # entered box\n",
    "            if p.person_id not in task2_ids:\n",
    "                task2_entered += 1\n",
    "                task2_ids.add(p.person_id)\n",
    "        else: # no overlap\n",
    "            # left box\n",
    "            if p.person_id in task2_ids:\n",
    "                task2_left += 1\n",
    "                task2_ids.remove(p.person_id)\n",
    "\n",
    "    ###############################\n",
    "    # [TASK 3 LOGIC]\n",
    "    groups = []\n",
    "    print(n)\n",
    "    groups = get_groups(cur_people)\n",
    "    groups, gID_count = set_group_ids(groups, prev_groups, gID_count)\n",
    "\n",
    "    # Print centroid ID in white\n",
    "    for p in cur_people:\n",
    "        p.color = (255,255,255)\n",
    "#         org = centroid(p.get_latest_ords()[:4])\n",
    "#         cv2.putText(cur_img_display, str(p.person_id), org, fontFace=cv2.FONT_HERSHEY_SIMPLEX, color=(255,255,255), fontScale=1, thickness=3)\n",
    "    \n",
    "    num_grouped_pedestrians = 0\n",
    "    # Color coordinate groups\n",
    "    for k in range(len(groups)):\n",
    "        num_grouped_pedestrians = num_grouped_pedestrians + len(groups[k].people)\n",
    "        for l in range(len(groups[k].people)):\n",
    "            groups[k].people[l].color = groups[k].color\n",
    "#             org = centroid(groups[k].people[l].get_latest_ords()[:4])\n",
    "#             cv2.putText(cur_img_display, str(groups[k].people[l].person_id), org, fontFace=cv2.FONT_HERSHEY_SIMPLEX, color=groups[k].color, fontScale=1, thickness=3)\n",
    "    \n",
    "    ###############################\n",
    "    # [GROUND TRUTH]\n",
    "    \n",
    "    \n",
    "    ###############################\n",
    "    # [PLOT FINALISED RESULTS]\n",
    "    # ** display user-selected bounding box (for Task 2)\n",
    "    cur_img_display = cv2.rectangle(cur_img_display, task2_roi[:2], \n",
    "                                    (task2_roi[0]+task2_roi[2], task2_roi[1]+task2_roi[3]), \n",
    "                                    (0,0,0), 1)\n",
    "    \n",
    "    if ENABLE_GROUND_TRUTH_DEBUG_VIEW:\n",
    "        cur_gt_dict = ground_truth[FRAME_START+n]\n",
    "        for cur_gt_dict_key in cur_gt_dict:\n",
    "            cur_bbox = cur_gt_dict[cur_gt_dict_key]\n",
    "            cur_bbox = tuple(map(int, cur_bbox))\n",
    "            cur_img_display = draw_rectangle(cur_img_display, cur_bbox, None, FONT, GROUND_TRUTH_COLOR, thickness=1)\n",
    "    \n",
    "    if ENABLE_TASK_VIEW:\n",
    "        # draw trajectories\n",
    "        for p in cur_people:\n",
    "            cur_img_display = p.draw_trajectory(cur_img_display)\n",
    "        # draw bboxes and labels on top\n",
    "        for p in cur_people:\n",
    "            cur_img_display = p.draw(cur_img_display)        \n",
    "\n",
    "    # [DISPLAY STATS]\n",
    "    # ** frame id\n",
    "    cv2.putText(cur_img_display, f'Fm {FRAME_START+n}', (0,20), FONT, 0.75, (255,0,0),2)\n",
    "    # ** Task 1c: pedestrians seen since frame 1\n",
    "    # ** Task 2\n",
    "    text_display = [\n",
    "        f'T1c: Total Pedestrns: {cur_person_id}',\n",
    "        f'T2a: Cur ROI Entries: {task2_entered}',\n",
    "        f'T2b: Cur ROI Depart: {task2_left}',\n",
    "        f'T2 : Cur ROI Inside : {len(task2_ids)}',\n",
    "        f'T3a : Pedestrns Alone: {len(cur_people) - num_grouped_pedestrians}',\n",
    "        f'T3a : Pedestrns Group: {num_grouped_pedestrians}'\n",
    "    ]\n",
    "    for cur_text_index, cur_text in enumerate(text_display): \n",
    "        cur_img_display = generic_text(cur_img_display, cur_text, (0,height-(len(text_display)-cur_text_index)*22))\n",
    "    \n",
    "    ###############################\n",
    "    # [ENABLE WINDOWS]\n",
    "    cv2.imshow(DETECT_WINDOW, cur_img_display)\n",
    "    if ENABLE_TRACKING_DEBUG_VIEW:\n",
    "        if n > 0:\n",
    "            flow_arrow = draw_flow(cur_img_gray, flow_res, step=8)\n",
    "            cv2.imshow(FLOW_WINDOW, flow_arrow)\n",
    "        else:\n",
    "            cv2.imshow(FLOW_WINDOW, cur_img_display)\n",
    "        cv2.imshow(MASK_WINDOW, cv2.bitwise_and(cur_img, cur_img, mask=bw_mask))\n",
    "\n",
    "    ###############################\n",
    "    # [BOOK-KEEPING FOR NEXT ITERATION]\n",
    "    # save results\n",
    "    video_out.write(cur_img_display)\n",
    "    \n",
    "    # setup for next iteration\n",
    "    prev_img_gray = cur_img_gray\n",
    "    prev_groups[n%num_prev_frames] = groups\n",
    "    \n",
    "    ###############################\n",
    "    # [PLAYBACK CONTROL]\n",
    "    if not is_interactive:\n",
    "        ch = cv2.waitKey(5) # millis\n",
    "    else:\n",
    "        while True:\n",
    "            ch = cv2.waitKey(0)\n",
    "            if ch in KEY_LIST:\n",
    "                break\n",
    "    if ch == ESC_KEY: # press ESC on the main window to exit\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "    elif ch == PAUSE_KEY:\n",
    "        is_interactive = not is_interactive\n",
    "    elif ch == NEXT_KEY:\n",
    "        pass # continue; do nothing\n",
    "\n",
    "# close results\n",
    "video_out.release()\n",
    "print('Video result saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
