# comp9517-project

This repository represents the work done by the METB team for the COMP9517 20T1 group project. Our members were:

 * Charles Ewart
 * Josh Basserabie
 * Victor Tse
 * William Miles
 * Zachary Hamid

## Setting up

You will need to install the following:

 * Python (3.7.4)
 * JupyterLab
 * Numpy (1.18.3)
 * OpenCV (3.4.2)
 * Scikit-learn (1.3.1)
 * Matplotlib (3.1.1)

### Dataset

You will also need to provide the directory for the PETS2009 S2.L1 dataset. This directory will need to be named `sequence` with images inside labelled consecutively from `000001.jpg` to `000795.jpg`.

The dataset may be available here: http://cs.binghamton.edu/~mrldata/pets2009

### YOLO Detection Model

The following files are required to be added to the root directory for YOLO detection to run:

 * yolov3.weights
 * yolov3.cfg
 * coco.names

These files can be retrieved from the following resources:

 * https://pjreddie.com/darknet/yolo/
 * https://github.com/pjreddie/darknet

## Running the code

Our implementation is located in `mean-shift.ipynb`, though we do not use the MeanShift algorithm in our code. Once this file has been opened in a Jupyter Notebook environment, all cells should be run, and a user interface generated by OpenCV will be displayed.

## User Interface Controls

After running the notebook file, you will be first presented with a Detection window that asks to select an ROI. Click and drag with the mouse cursor to create a bounding box. If you are unable with the result, you may redraw the bounding box by clicking and dragging again. Once you are happy, press ENTER to begin the playback.

Playback controls are as follows:

 * ESC: press this to exit the playback and close the program
 * 'p': press this to play/pause the playback
 * 'n': when the playback has been paused, press this to go forward 1 frame

You may need to wait 1-2s per frame update due to on-the-fly processing demands. Generally, the interface will be very responsive to user inputs as OpenCV is able to automatically buffer them.

Upon exiting the program, a video of the playback will be created at 7 FPS. This will be generated under `tracking-output.avi`.

## User Interface View

The main Detection window display will show the following:

 * User-selected ROI: as a static black box
 * Detection results (per-frame): as blue boxes
 * Tracking results (per-frame): as red boxes
 * Ground truths: as black boxes
 * Trajectories: white trajectories will be shown for all pedestrians not in a group, whilst pedestrians in the same group will be assigned a common random colour.
 * Frame counter: provided top-left of screen. This counts upwards from 0.

Each detected/tracked pedestrian is given a unique pedestrian ID that is displayed at the top-right of their bounding box.

Project task statistics are also shown on the bottom-left. They denote the following:

 * Total pedestrians: for Task 1c
 * Current ROI entries: for Task 2a
 * Current ROI departures: for Task 2b
 * Current ROI inside: given that we have defined entries/departures as single-frame events, this counter displays the number of pedestrians that have remained inside the ROI after initial entry.
 * Pedestrians alone: counts the number of pedestrians not in a group. Aids evaluation of Task 3.
 * Pedestrians in a group: counts the number of pedestrians in a group. Aids evaluation of Task 3.